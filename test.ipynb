{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the model\n",
    "model_dict = pickle.load(open(\"./model.p\", \"rb\"))\n",
    "model = model_dict[\"model\"]\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Labels dictionary\n",
    "labels_dict = {0: \"1\", 1: \"2\", 2: \"3\"}\n",
    "\n",
    "# Specify image path directly here\n",
    "image_path = r\"C:\\Repositories\\Sign-Language-Detection\\data\\2\\3.jpg\"  # CHANGE THIS TO YOUR IMAGE PATH\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    print(f\"Error loading image: {image_path}\")\n",
    "    exit()\n",
    "\n",
    "# Get image dimensions\n",
    "H, W, _ = image.shape\n",
    "\n",
    "# Convert to RGB for MediaPipe\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Process the image\n",
    "results = hands.process(image_rgb)\n",
    "\n",
    "# Prepare data storage\n",
    "data_aux = []\n",
    "x_ = []\n",
    "y_ = []\n",
    "\n",
    "# Process hand landmarks if detected\n",
    "if results.multi_hand_landmarks:\n",
    "    # Draw landmarks\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style(),\n",
    "        )\n",
    "    \n",
    "    # Process the first detected hand only\n",
    "    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "    \n",
    "    # First collect all coordinates\n",
    "    for i in range(len(hand_landmarks.landmark)):\n",
    "        x = hand_landmarks.landmark[i].x\n",
    "        y = hand_landmarks.landmark[i].y\n",
    "        x_.append(x)\n",
    "        y_.append(y)\n",
    "    \n",
    "    # Normalize feature vector using min and max values\n",
    "    x_min, x_max = min(x_), max(x_)\n",
    "    y_min, y_max = min(y_), max(y_)\n",
    "    for i in range(len(hand_landmarks.landmark)):\n",
    "        x = hand_landmarks.landmark[i].x\n",
    "        y = hand_landmarks.landmark[i].y\n",
    "        data_aux.append((x - x_min) / (x_max - x_min))\n",
    "        data_aux.append((y - y_min) / (y_max - y_min))\n",
    "\n",
    "    # Debugging outputs\n",
    "    print(f\"Feature vector: {data_aux}\")\n",
    "    print(f\"Feature vector length: {len(data_aux)}\")\n",
    "    \n",
    "    # Draw bounding box\n",
    "    x1 = int(min(x_) * W) - 10\n",
    "    y1 = int(min(y_) * H) - 10\n",
    "    x2 = int(max(x_) * W) + 10\n",
    "    y2 = int(max(y_) * H) + 10\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict([np.asarray(data_aux)])\n",
    "    predicted_character = labels_dict[int(prediction[0])]\n",
    "    \n",
    "    # Add prediction text\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        f\"Prediction: {predicted_character}\",\n",
    "        (x1, y1 - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.9,\n",
    "        (0, 0, 255),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f\"Image: {image_path}\")\n",
    "    print(f\"Prediction: {predicted_character}\")\n",
    "    print(f\"Feature vector length: {len(data_aux)}\")\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Prediction: {predicted_character}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No hand detected in {image_path}\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"No hand detected\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
